{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCFw//ZW7LlndoURcxzTVY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexKalll/Supervised-ML-Models/blob/main/Model_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SECTION 0: Imports & Global Configuration\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.quantization as tq\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Explicitly set the quantization engine to QNNPACK\n",
        "if 'qnnpack' in torch.backends.quantized.supported_engines:\n",
        "    torch.backends.quantized.engine = 'qnnpack'\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 2\n",
        "KD_EPOCHS = 2\n",
        "LR = 1e-3\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 1: Data Loading (MNIST)\n",
        "# ============================================================\n",
        "\n",
        "def get_dataloaders():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 2: Baseline Feed-Forward Network\n",
        "# ============================================================\n",
        "\n",
        "class BaselineFFN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 3: Mixture of Experts (MoE)\n",
        "# ============================================================\n",
        "\n",
        "class MoELayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_experts=4):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "        # Helper stubs to create a \"Float Island\"\n",
        "        self.dequant = tq.DeQuantStub()\n",
        "        self.quant = tq.QuantStub()\n",
        "\n",
        "        self.gate = nn.Linear(input_dim, num_experts)\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(input_dim, output_dim),\n",
        "                nn.ReLU()\n",
        "            ) for _ in range(num_experts)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Dequantize input to float (Float Island begins)\n",
        "        x_float = self.dequant(x)\n",
        "\n",
        "        # 2. Gate (Float op)\n",
        "        gate_logits = self.gate(x_float)\n",
        "        gate_probs = F.softmax(gate_logits, dim=1)\n",
        "\n",
        "        # 3. Experts (Float op)\n",
        "        expert_outputs = torch.stack([expert(x_float) for expert in self.experts], dim=1)\n",
        "\n",
        "        # 4. Weighted Sum (Float op)\n",
        "        output_float = torch.sum(gate_probs.unsqueeze(-1) * expert_outputs, dim=1)\n",
        "\n",
        "        # 5. Quantize output back to int8 (Float Island ends)\n",
        "        output_quantized = self.quant(output_float)\n",
        "\n",
        "        return output_quantized\n",
        "\n",
        "\n",
        "class MoEFFN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.quant = tq.QuantStub()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.moe = MoELayer(256, 128, num_experts=4)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.dequant = tq.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.moe(x) # Handles internal dequant/quant\n",
        "        x = self.fc3(x)\n",
        "        x = self.dequant(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 4: Training & Evaluation Utils\n",
        "# ============================================================\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(x), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, device=None):\n",
        "    if device is None: device = DEVICE\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            preds = model(x).argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "def measure_latency(model, dataloader, num_batches=50, device=None):\n",
        "    if device is None: device = DEVICE\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    # Warmup\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(dataloader):\n",
        "            model(x.to(device))\n",
        "            if i >= 5: break\n",
        "    # Measure\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(dataloader):\n",
        "            model(x.to(device))\n",
        "            if i >= num_batches: break\n",
        "    end = time.time()\n",
        "    return (end - start) / num_batches\n",
        "\n",
        "def count_parameters(model):\n",
        "    total_params = 0\n",
        "    for name, parameter in model.state_dict().items():\n",
        "        if isinstance(parameter, torch.Tensor):\n",
        "            is_weight_or_bias = (\"weight\" in name or \"bias\" in name)\n",
        "            is_not_metadata = (\"scale\" not in name and \"zero_point\" not in name)\n",
        "            if is_weight_or_bias and is_not_metadata:\n",
        "                total_params += parameter.numel()\n",
        "    return total_params\n",
        "\n",
        "def get_model_size_mb(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size_mb = os.path.getsize(\"temp.p\") / (1024 * 1024)\n",
        "    os.remove(\"temp.p\")\n",
        "    return size_mb\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 5: PTQ & QAT Helpers\n",
        "# ============================================================\n",
        "\n",
        "def apply_ptq_moe(model, calibration_loader):\n",
        "    \"\"\"Post-Training Quantization for MoE\"\"\"\n",
        "    model.eval()\n",
        "    model.qconfig = tq.get_default_qconfig(\"qnnpack\")\n",
        "    model_q = model.to(\"cpu\")\n",
        "\n",
        "    # Disable quantization for Float Island\n",
        "    model_q.moe.gate.qconfig = None\n",
        "    for expert in model_q.moe.experts:\n",
        "        expert[0].qconfig = None\n",
        "        expert[1].qconfig = None\n",
        "\n",
        "    tq.fuse_modules(model_q, [[\"fc1\", \"relu1\"]], inplace=True)\n",
        "    tq.prepare(model_q, inplace=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, _ in calibration_loader:\n",
        "            model_q(x.to(\"cpu\"))\n",
        "\n",
        "    tq.convert(model_q, inplace=True)\n",
        "    return model_q\n",
        "\n",
        "def apply_qat_moe(train_loader, test_loader):\n",
        "    \"\"\"Quantization Aware Training for MoE\"\"\"\n",
        "    print(\"\\nPreparing MoE for QAT...\")\n",
        "\n",
        "    # 1. Initialize a fresh model\n",
        "    qat_model = MoEFFN().to(DEVICE)\n",
        "\n",
        "    # 2. Set QAT Configuration\n",
        "    qat_model.qconfig = tq.get_default_qat_qconfig(\"qnnpack\")\n",
        "\n",
        "    # 3. Disable quantization for the Float Island (Experts & Router)\n",
        "    # We want these to remain high-precision during training and inference\n",
        "    qat_model.moe.gate.qconfig = None\n",
        "    for expert in qat_model.moe.experts:\n",
        "        expert[0].qconfig = None\n",
        "        expert[1].qconfig = None\n",
        "\n",
        "    # 4. Fuse Modules (fc1 + relu1)\n",
        "    tq.fuse_modules(qat_model, [[\"fc1\", \"relu1\"]], inplace=True)\n",
        "\n",
        "    # 5. Prepare for QAT (Inserts fake quantization nodes)\n",
        "    tq.prepare_qat(qat_model, inplace=True)\n",
        "\n",
        "    # 6. Train (Fine-tune with fake quantization noise)\n",
        "    optimizer = optim.Adam(qat_model.parameters(), lr=1e-4) # Lower LR for QAT\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        qat_model.train() # Important: QAT happens in train mode\n",
        "        total_loss = 0\n",
        "        for x, y in tqdm(train_loader, desc=f\"QAT Epoch {epoch+1}\", leave=False):\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = qat_model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"QAT Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    # 7. Convert to actual INT8 model (Move to CPU first)\n",
        "    qat_model.eval()\n",
        "    qat_model = qat_model.to(\"cpu\")\n",
        "    tq.convert(qat_model, inplace=True)\n",
        "\n",
        "    return qat_model\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 6: Knowledge Distillation\n",
        "# ============================================================\n",
        "\n",
        "def train_kd(student, teacher, dataloader, optimizer, alpha=0.7, temperature=4.0):\n",
        "    student.train()\n",
        "    teacher.eval()\n",
        "    total_loss = 0\n",
        "    for x, y in tqdm(dataloader, desc=\"KD Training\", leave=False):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = teacher(x)\n",
        "        student_logits = student(x)\n",
        "\n",
        "        soft_loss = F.kl_div(\n",
        "            F.log_softmax(student_logits/temperature, dim=1),\n",
        "            F.softmax(teacher_logits/temperature, dim=1),\n",
        "            reduction=\"batchmean\"\n",
        "        ) * (temperature**2)\n",
        "        hard_loss = F.cross_entropy(student_logits, y)\n",
        "\n",
        "        loss = alpha*soft_loss + (1-alpha)*hard_loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 7: Main Experiment Pipeline\n",
        "# ============================================================\n",
        "\n",
        "def main():\n",
        "    train_loader, test_loader = get_dataloaders()\n",
        "\n",
        "    # Calibration loader (subset of train data)\n",
        "    calibration_subset = Subset(train_loader.dataset, range(1024))\n",
        "    calibration_loader = DataLoader(calibration_subset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. Baseline Training\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\n=== 1. Training Baseline ===\")\n",
        "    baseline = BaselineFFN().to(DEVICE)\n",
        "    opt = optim.Adam(baseline.parameters(), lr=LR)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        loss = train(baseline, train_loader, opt, criterion)\n",
        "        print(f\"Epoch {epoch+1}: Loss={loss:.4f}\")\n",
        "\n",
        "    base_acc = evaluate(baseline, test_loader)\n",
        "    base_lat = measure_latency(baseline, test_loader)\n",
        "\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. MoE Training (Scratch)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\n=== 2. Training MoE ===\")\n",
        "    moe = MoEFFN().to(DEVICE)\n",
        "    opt_moe = optim.Adam(moe.parameters(), lr=LR)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        loss = train(moe, train_loader, opt_moe, criterion)\n",
        "        print(f\"Epoch {epoch+1}: Loss={loss:.4f}\")\n",
        "\n",
        "    moe_acc = evaluate(moe, test_loader)\n",
        "    moe_lat = measure_latency(moe, test_loader)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. MoE + PTQ\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\n=== 3. Applying PTQ to MoE ===\")\n",
        "    moe_ptq_model = MoEFFN()\n",
        "    moe_ptq_model.load_state_dict(moe.state_dict())\n",
        "    moe_ptq_model = apply_ptq_moe(moe_ptq_model, calibration_loader)\n",
        "\n",
        "    moe_ptq_acc = evaluate(moe_ptq_model, test_loader, device=\"cpu\")\n",
        "    moe_ptq_lat = measure_latency(moe_ptq_model, test_loader, device=\"cpu\")\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 4. MoE + QAT (New!)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\n=== 4. Training MoE with QAT ===\")\n",
        "    # QAT trains a fresh model from scratch (or fine-tunes)\n",
        "    moe_qat_model = apply_qat_moe(train_loader, test_loader)\n",
        "\n",
        "    moe_qat_acc = evaluate(moe_qat_model, test_loader, device=\"cpu\")\n",
        "    moe_qat_lat = measure_latency(moe_qat_model, test_loader, device=\"cpu\")\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 5. MoE + KD Training\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\n=== 5. Training MoE with Knowledge Distillation ===\")\n",
        "    kd_student = MoEFFN().to(DEVICE)\n",
        "    opt_kd = optim.Adam(kd_student.parameters(), lr=LR)\n",
        "\n",
        "    for epoch in range(KD_EPOCHS):\n",
        "        loss = train_kd(kd_student, baseline, train_loader, opt_kd)\n",
        "        print(f\"KD Epoch {epoch+1}: Loss={loss:.4f}\")\n",
        "\n",
        "    kd_acc = evaluate(kd_student, test_loader)\n",
        "    kd_lat = measure_latency(kd_student, test_loader)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 6. MoE + KD + PTQ\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\n=== 6. Applying PTQ to MoE (KD) ===\")\n",
        "    kd_ptq_model = MoEFFN()\n",
        "    kd_ptq_model.load_state_dict(kd_student.state_dict())\n",
        "    kd_ptq_model = apply_ptq_moe(kd_ptq_model, calibration_loader)\n",
        "\n",
        "    kd_ptq_acc = evaluate(kd_ptq_model, test_loader, device=\"cpu\")\n",
        "    kd_ptq_lat = measure_latency(kd_ptq_model, test_loader, device=\"cpu\")\n",
        "\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # FINAL RESULTS\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\n\" + \"=\"*85)\n",
        "    print(f\"{'Model':<20} | {'Acc':<8} | {'Lat(ms)':<8} | {'Params':<12} | {'Size(MB)':<10}\")\n",
        "    print(\"-\" * 85)\n",
        "\n",
        "    def print_row(name, acc, lat, model):\n",
        "        size = get_model_size_mb(model)\n",
        "        params = count_parameters(model)\n",
        "        print(f\"{name:<20} | {acc:.4f}   | {lat*1000:.2f}     | {params:<12,} | {size:.2f}\")\n",
        "\n",
        "    print_row(\"Baseline\", base_acc, base_lat, baseline)\n",
        "    print_row(\"MoE\", moe_acc, moe_lat, moe)\n",
        "    print_row(\"MoE + PTQ\", moe_ptq_acc, moe_ptq_lat, moe_ptq_model)\n",
        "    print_row(\"MoE + QAT\", moe_qat_acc, moe_qat_lat, moe_qat_model)\n",
        "    print_row(\"MoE + KD\", kd_acc, kd_lat, kd_student)\n",
        "    print_row(\"MoE + KD + PTQ\", kd_ptq_acc, kd_ptq_lat, kd_ptq_model)\n",
        "\n",
        "    print(\"=\"*85)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SYQeT0v7bt0",
        "outputId": "f826f898-a71e-4368-fae1-4174365ca688"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 1. Training Baseline ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss=0.2278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss=0.0912\n",
            "\n",
            "=== 2. Training MoE ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss=0.2370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss=0.0949\n",
            "\n",
            "=== 3. Applying PTQ to MoE ===\n",
            "\n",
            "=== 4. Training MoE with QAT ===\n",
            "\n",
            "Preparing MoE for QAT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QAT Epoch 1: Loss=0.6094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QAT Epoch 2: Loss=0.1937\n",
            "\n",
            "=== 5. Training MoE with Knowledge Distillation ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 1: Loss=0.6862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 2: Loss=0.1100\n",
            "\n",
            "=== 6. Applying PTQ to MoE (KD) ===\n",
            "\n",
            "=====================================================================================\n",
            "Model                | Acc      | Lat(ms)  | Params       | Size(MB)  \n",
            "-------------------------------------------------------------------------------------\n",
            "Baseline             | 0.9712   | 14.79     | 235,146      | 0.90\n",
            "MoE                  | 0.9703   | 17.99     | 334,862      | 1.28\n",
            "MoE + PTQ            | 0.9698   | 17.71     | 132,612      | 0.71\n",
            "MoE + QAT            | 0.9509   | 18.16     | 132,612      | 0.71\n",
            "MoE + KD             | 0.9712   | 15.17     | 334,862      | 1.28\n",
            "MoE + KD + PTQ       | 0.9704   | 18.90     | 132,612      | 0.71\n",
            "=====================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN6CA8-31bHU",
        "outputId": "4b9e3479-ac3a-4a6c-88f1-c47ed57a2065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training Baseline ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:12<00:00, 72.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss=0.2881, Acc=0.9604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:13<00:00, 70.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss=0.1058, Acc=0.9704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:13<00:00, 68.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Loss=0.0706, Acc=0.9749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:14<00:00, 66.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Loss=0.0524, Acc=0.9791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:13<00:00, 67.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Loss=0.0411, Acc=0.9775\n",
            "\n",
            "=== Training MoE ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:15<00:00, 59.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss=0.2725, Acc=0.9611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:16<00:00, 57.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss=0.1028, Acc=0.9724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:15<00:00, 59.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Loss=0.0705, Acc=0.9729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:16<00:00, 58.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Loss=0.0512, Acc=0.9731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:16<00:00, 57.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Loss=0.0395, Acc=0.9749\n",
            "\n",
            "=== Applying PTQ to Baseline ===\n",
            "\n",
            "=== Training MoE with Knowledge Distillation ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:17<00:00, 52.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 1: Loss=1.5981, Acc=0.9597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:17<00:00, 54.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 2: Loss=0.2881, Acc=0.9713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:17<00:00, 52.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 3: Loss=0.1462, Acc=0.9739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:17<00:00, 54.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 4: Loss=0.1000, Acc=0.9759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:18<00:00, 51.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 5: Loss=0.0783, Acc=0.9778\n",
            "\n",
            "===== FINAL RESULTS ===\n",
            "Baseline      | Acc: 0.9775 | Lat: 0.008784 | Params: 235,146 | Size(MB): 0.90\n",
            "Baseline + PTQ| Acc: 0.9779 | Lat: 0.011063 | Params: 0 | Size(MB): 0.00\n",
            "MoE           | Acc: 0.9749 | Lat: 0.008887 | Params: 334,862 | Size(MB): 1.28\n",
            "MoE + KD      | Acc: 0.9778 | Lat: 0.009381 | Params: 334,862 | Size(MB): 1.28\n",
            "MoE + KD + PTQ| Acc: 0.9777 | Lat: 0.014814 | Params: 132,612 | Size(MB): 0.13\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SECTION 0: Imports & Global Configuration\n",
        "# ============================================================\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch.quantization as tq\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "  DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "KD_EPOCHS = 5\n",
        "LR = 1e-3\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 1: Data Loading (MNIST)\n",
        "# ============================================================\n",
        "\n",
        "def get_dataloaders():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.MNIST(\n",
        "        root=\"./data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    test_dataset = datasets.MNIST(\n",
        "        root=\"./data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 2: Baseline Feed-Forward Network\n",
        "# ============================================================\n",
        "\n",
        "class BaselineFFN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.quant = tq.QuantStub() # Added for PTQ\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.relu1 = nn.ReLU() # Changed from F.relu for fusibility\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.relu2 = nn.ReLU() # Changed from F.relu for fusibility\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.dequant = tq.DeQuantStub() # Added for PTQ\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x) # Added for PTQ\n",
        "        x = x.view(x.size(0), -1)  # flatten\n",
        "        x = self.relu1(self.fc1(x)) # Use nn.ReLU instance\n",
        "        x = self.relu2(self.fc2(x)) # Use nn.ReLU instance\n",
        "        x = self.fc3(x)\n",
        "        x = self.dequant(x) # Added for PTQ\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 3: Mixture of Experts (MoE)\n",
        "# ============================================================\n",
        "\n",
        "class MoELayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_experts=4):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "        # Quantization stubs for the MoE Layer itself to define its boundaries\n",
        "        self.dequant = tq.DeQuantStub() # Dequantize input when entering MoELayer\n",
        "        self.quant = tq.QuantStub() # Quantize output when exiting MoELayer\n",
        "\n",
        "        # Router (gate) - these layers will operate on float inputs after dequantization\n",
        "        self.gate = nn.Linear(input_dim, num_experts)\n",
        "\n",
        "        # Experts - these layers will operate on float inputs after dequantization\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(input_dim, output_dim),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "            for _ in range(num_experts)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Assume input 'x' is a quantized tensor coming from a previous layer (e.g., fc1 in MoEFFN)\n",
        "        # Dequantize the input for internal float operations within MoELayer\n",
        "        x_float = self.dequant(x)\n",
        "\n",
        "        # Routing probabilities (these operations will happen on float tensors)\n",
        "        gate_logits = self.gate(x_float)\n",
        "        gate_probs = F.softmax(gate_logits, dim=1)\n",
        "\n",
        "        # Expert outputs (each expert takes float input and produces float output)\n",
        "        expert_outputs_list = []\n",
        "        for expert in self.experts:\n",
        "            expert_outputs_list.append(expert(x_float))\n",
        "\n",
        "        expert_outputs = torch.stack(\n",
        "            expert_outputs_list,\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        # Weighted sum (these operations will happen on float tensors)\n",
        "        output_float = torch.sum(\n",
        "            gate_probs.unsqueeze(-1) * expert_outputs,\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        # Quantize the output of MoELayer before passing it to the next quantized layer (e.g., fc3 in MoEFFN)\n",
        "        output_quantized = self.quant(output_float)\n",
        "\n",
        "        return output_quantized\n",
        "\n",
        "\n",
        "class MoEFFN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.quant = tq.QuantStub() # Quantization stub for the model's input\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.relu1 = nn.ReLU() # Changed from F.relu for fusibility\n",
        "        self.moe = MoELayer(256, 128, num_experts=4) # MoELayer itself handles quant/dequant internally\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.dequant = tq.DeQuantStub() # Dequantization stub for the model's output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x) # Quantize input\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu1(self.fc1(x)) # fc1 output -> relu1 output (will be quantized after fusion)\n",
        "        x = self.moe(x) # MoELayer takes quantized input, outputs quantized\n",
        "        x = self.fc3(x) # fc3 takes quantized input, outputs quantized\n",
        "        x = self.dequant(x) # Dequantize final output\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 4: Standard Training & Evaluation\n",
        "# ============================================================\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in tqdm(dataloader):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device=None):\n",
        "    if device is None:\n",
        "        device = DEVICE\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 5: Latency Measurement\n",
        "# ============================================================\n",
        "\n",
        "def measure_latency(model, dataloader, num_batches=50, device=None):\n",
        "    if device is None:\n",
        "        device = DEVICE\n",
        "    model.eval()\n",
        "\n",
        "    # Warm-up\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(dataloader):\n",
        "            model(x.to(device))\n",
        "            if i >= 5:\n",
        "                break\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(dataloader):\n",
        "            model(x.to(device))\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "\n",
        "    end = time.time()\n",
        "    return (end - start) / num_batches\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 5.5: Model Size & Parameter Counting\n",
        "# ============================================================\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def estimate_model_size(model, quantized=False):\n",
        "    \"\"\"\n",
        "    Rough model size estimation:\n",
        "    FP32 -> 4 bytes per parameter\n",
        "    INT8 -> 1 byte per parameter\n",
        "    \"\"\"\n",
        "    num_params = count_parameters(model)\n",
        "    bytes_per_param = 1 if quantized else 4\n",
        "    size_mb = (num_params * bytes_per_param) / (1024 ** 2)\n",
        "    return size_mb\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 6: Post-Training Quantization (PTQ)\n",
        "# ============================================================\n",
        "\n",
        "def apply_ptq(model, calibration_loader):\n",
        "    model.eval()\n",
        "    model.qconfig = tq.get_default_qconfig(\"qnnpack\") # Changed from fbgemm\n",
        "\n",
        "    # Move model to CPU before preparing and converting for FBGEMM,\n",
        "    # as FBGEMM typically uses CPU-optimized kernels for x86.\n",
        "    model_for_quant = model.to(torch.device(\"cpu\"))\n",
        "\n",
        "    # Explicitly disable quantization for the internal modules of MoELayer\n",
        "    # as it's designed to operate as a float island internally.\n",
        "    model_for_quant.moe.gate.qconfig = None\n",
        "    for expert_seq in model_for_quant.moe.experts:\n",
        "        expert_seq[0].qconfig = None  # nn.Linear in expert\n",
        "        expert_seq[1].qconfig = None  # nn.ReLU in expert\n",
        "\n",
        "    # Fuse modules for better quantization performance.\n",
        "    # For MoEFFN's fc1 and relu1:\n",
        "    tq.fuse_modules(\n",
        "        model_for_quant,\n",
        "        [[\"fc1\", \"relu1\"]],\n",
        "        inplace=True\n",
        "    )\n",
        "    # MoELayer's internal layers (gate, experts) are now designed to operate in float\n",
        "    # due to explicit dequant/quant stubs in MoELayer, so no fusion is needed for them.\n",
        "\n",
        "    tq.prepare(model_for_quant, inplace=True);\n",
        "\n",
        "    # Calibration on CPU with CPU tensors\n",
        "    with torch.no_grad():\n",
        "        for x, _ in calibration_loader:\n",
        "            model_for_quant(x.to(torch.device(\"cpu\")))\n",
        "\n",
        "    tq.convert(model_for_quant, inplace=True)\n",
        "    return model_for_quant # Return the CPU-quantized model\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 7: Knowledge Distillation Training\n",
        "# ============================================================\n",
        "\n",
        "def train_kd(student, teacher, dataloader, optimizer, alpha=0.7, temperature=4.0):\n",
        "    student.train()\n",
        "    teacher.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in tqdm(dataloader):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = teacher(x)\n",
        "\n",
        "        student_logits = student(x)\n",
        "\n",
        "        hard_loss = F.cross_entropy(student_logits, y)\n",
        "\n",
        "        soft_loss = F.kl_div(\n",
        "            F.log_softmax(student_logits / temperature, dim=1),\n",
        "            F.softmax(teacher_logits / temperature, dim=1),\n",
        "            reduction=\"batchmean\"\n",
        "        ) * (temperature ** 2)\n",
        "\n",
        "        loss = alpha * soft_loss + (1 - alpha) * hard_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 8: Main Experiment Pipeline\n",
        "# ============================================================\n",
        "\n",
        "def main():\n",
        "    # Explicitly set the quantization engine for CPU\n",
        "    torch.backends.quantized.engine = 'qnnpack' # Changed from fbgemm\n",
        "\n",
        "    train_loader, test_loader = get_dataloaders()\n",
        "\n",
        "    # Define calibration_loader early, as it's used in multiple sections.\n",
        "    calibration_subset = Subset(train_loader.dataset, range(1024))\n",
        "    calibration_loader = DataLoader(calibration_subset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # ---- Baseline ----\n",
        "    print(\"\\n=== Training Baseline ===\")\n",
        "    baseline = BaselineFFN().to(DEVICE)\n",
        "    opt = optim.Adam(baseline.parameters(), lr=LR)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        loss = train(baseline, train_loader, opt, criterion)\n",
        "        acc = evaluate(baseline, test_loader, device=DEVICE)\n",
        "        print(f\"Epoch {epoch+1}: Loss={loss:.4f}, Acc={acc:.4f}\")\n",
        "\n",
        "    base_acc = evaluate(baseline, test_loader, device=DEVICE)\n",
        "    base_lat = measure_latency(baseline, test_loader, device=DEVICE)\n",
        "\n",
        "    # ---- MoE ----\n",
        "    print(\"\\n=== Training MoE ===\")\n",
        "    moe = MoEFFN().to(DEVICE)\n",
        "    opt = optim.Adam(moe.parameters(), lr=LR)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        loss = train(moe, train_loader, opt, criterion)\n",
        "        acc = evaluate(moe, test_loader, device=DEVICE)\n",
        "        print(f\"Epoch {epoch+1}: Loss={loss:.4f}, Acc={acc:.4f}\")\n",
        "\n",
        "    moe_acc = evaluate(moe, test_loader, device=DEVICE)\n",
        "    moe_lat = measure_latency(moe, test_loader, device=DEVICE)\n",
        "\n",
        "    # ---- Baseline + PTQ (PTQ-only control) ----\n",
        "    print(\"\\n=== Applying PTQ to Baseline ===\")\n",
        "\n",
        "    baseline_for_quant = BaselineFFN()\n",
        "    baseline_for_quant.load_state_dict(baseline.state_dict())\n",
        "\n",
        "    # Baseline does NOT need MoE-specific handling\n",
        "    baseline_for_quant.qconfig = tq.get_default_qconfig(\"qnnpack\") # Changed from fbgemm\n",
        "    baseline_for_quant = baseline_for_quant.to(torch.device(\"cpu\"))\n",
        "\n",
        "    tq.fuse_modules(\n",
        "        baseline_for_quant,\n",
        "        [[\"fc1\", \"relu1\"], [\"fc2\", \"relu2\"]], # Restored both fusions\n",
        "        inplace=True\n",
        "    )\n",
        "\n",
        "    tq.prepare(baseline_for_quant, inplace=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, _ in calibration_loader:\n",
        "            baseline_for_quant(x.to(torch.device(\"cpu\")))\n",
        "\n",
        "    tq.convert(baseline_for_quant, inplace=True)\n",
        "\n",
        "    baseline_ptq_acc = evaluate(\n",
        "        baseline_for_quant,\n",
        "        test_loader,\n",
        "        device=torch.device(\"cpu\")\n",
        "    )\n",
        "\n",
        "    baseline_ptq_lat = measure_latency(\n",
        "        baseline_for_quant,\n",
        "        test_loader,\n",
        "        device=torch.device(\"cpu\")\n",
        "    )\n",
        "\n",
        "\n",
        "    # ---- Knowledge Distillation ----\n",
        "    print(\"\\n=== Training MoE with Knowledge Distillation ===\")\n",
        "    kd_student = MoEFFN().to(DEVICE)\n",
        "    opt = optim.Adam(kd_student.parameters(), lr=LR)\n",
        "\n",
        "    for epoch in range(KD_EPOCHS):\n",
        "        loss = train_kd(kd_student, baseline, train_loader, opt)\n",
        "        acc = evaluate(kd_student, test_loader, device=DEVICE)\n",
        "        print(f\"KD Epoch {epoch+1}: Loss={loss:.4f}, Acc={acc:.4f}\")\n",
        "\n",
        "    kd_acc = evaluate(kd_student, test_loader, device=DEVICE)\n",
        "    kd_lat = measure_latency(kd_student, test_loader, device=DEVICE)\n",
        "\n",
        "    # ---- PTQ ----\n",
        "    # calibration_subset = Subset(train_loader.dataset, range(1024)) # Moved up\n",
        "    # calibration_loader = DataLoader(calibration_subset, batch_size=BATCH_SIZE) # Moved up\n",
        "\n",
        "    # Create a new MoEFFN instance for quantization, as applying PTQ modifies the model in-place\n",
        "    # and we want to quantize the KD student, not the original MoE model.\n",
        "    # Also, move to CPU before applying PTQ and fusion.\n",
        "    quant_kd_student = MoEFFN()\n",
        "    quant_kd_student.load_state_dict(kd_student.state_dict())\n",
        "    quant_moe = apply_ptq(quant_kd_student, calibration_loader)\n",
        "\n",
        "    quant_acc = evaluate(quant_moe, test_loader, device=torch.device(\"cpu\"))\n",
        "    quant_lat = measure_latency(quant_moe, test_loader, device=torch.device(\"cpu\"))\n",
        "\n",
        "\n",
        "    print(\"\\n===== FINAL RESULTS ===\")\n",
        "\n",
        "    print(f\"Baseline      | Acc: {base_acc:.4f} | Lat: {base_lat:.6f} | \"\n",
        "          f\"Params: {count_parameters(baseline):,} | \"\n",
        "          f\"Size(MB): {estimate_model_size(baseline):.2f}\")\n",
        "\n",
        "    print(f\"Baseline + PTQ| Acc: {baseline_ptq_acc:.4f} | Lat: {baseline_ptq_lat:.6f} | \"\n",
        "          f\"Params: {count_parameters(baseline_for_quant):,} | \"\n",
        "          f\"Size(MB): {estimate_model_size(baseline_for_quant, quantized=True):.2f}\")\n",
        "\n",
        "    print(f\"MoE           | Acc: {moe_acc:.4f} | Lat: {moe_lat:.6f} | \"\n",
        "          f\"Params: {count_parameters(moe):,} | \"\n",
        "          f\"Size(MB): {estimate_model_size(moe):.2f}\")\n",
        "\n",
        "    print(f\"MoE + KD      | Acc: {kd_acc:.4f} | Lat: {kd_lat:.6f} | \"\n",
        "          f\"Params: {count_parameters(kd_student):,} | \"\n",
        "          f\"Size(MB): {estimate_model_size(kd_student):.2f}\")\n",
        "\n",
        "    print(f\"MoE + KD + PTQ| Acc: {quant_acc:.4f} | Lat: {quant_lat:.6f} | \"\n",
        "          f\"Params: {count_parameters(quant_moe):,} | \"\n",
        "          f\"Size(MB): {estimate_model_size(quant_moe, quantized=True):.2f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SECTION 0: Imports & Global Configuration\n",
        "# ============================================================\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch.quantization as tq\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "  DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "KD_EPOCHS = 5\n",
        "LR = 1e-3\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 1: Data Loading (MNIST)\n",
        "# ============================================================\n",
        "\n",
        "def get_dataloaders():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.MNIST(\n",
        "        root=\"./data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    test_dataset = datasets.MNIST(\n",
        "        root=\"./data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 2: Baseline Feed-Forward Network\n",
        "# ============================================================\n",
        "\n",
        "class BaselineFFN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.relu1 = nn.ReLU() # Changed from F.relu for fusibility\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.relu2 = nn.ReLU() # Changed from F.relu for fusibility\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # flatten\n",
        "        x = self.relu1(self.fc1(x)) # Use nn.ReLU instance\n",
        "        x = self.relu2(self.fc2(x)) # Use nn.ReLU instance\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 3: Mixture of Experts (MoE)\n",
        "# ============================================================\n",
        "\n",
        "class MoELayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_experts=4):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "        # Quantization stubs for the MoE Layer itself to define its boundaries\n",
        "        self.dequant = tq.DeQuantStub() # Dequantize input when entering MoELayer\n",
        "        self.quant = tq.QuantStub() # Quantize output when exiting MoELayer\n",
        "\n",
        "        # Router (gate) - these layers will operate on float inputs after dequantization\n",
        "        self.gate = nn.Linear(input_dim, num_experts)\n",
        "\n",
        "        # Experts - these layers will operate on float inputs after dequantization\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(input_dim, output_dim),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "            for _ in range(num_experts)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Assume input 'x' is a quantized tensor coming from a previous layer (e.g., fc1 in MoEFFN)\n",
        "        # Dequantize the input for internal float operations within MoELayer\n",
        "        x_float = self.dequant(x)\n",
        "\n",
        "        # Routing probabilities (these operations will happen on float tensors)\n",
        "        gate_logits = self.gate(x_float)\n",
        "        gate_probs = F.softmax(gate_logits, dim=1)\n",
        "\n",
        "        # Expert outputs (each expert takes float input and produces float output)\n",
        "        expert_outputs_list = []\n",
        "        for expert in self.experts:\n",
        "            expert_outputs_list.append(expert(x_float))\n",
        "\n",
        "        expert_outputs = torch.stack(\n",
        "            expert_outputs_list,\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        # Weighted sum (these operations will happen on float tensors)\n",
        "        output_float = torch.sum(\n",
        "            gate_probs.unsqueeze(-1) * expert_outputs,\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        # Quantize the output of MoELayer before passing it to the next quantized layer (e.g., fc3 in MoEFFN)\n",
        "        output_quantized = self.quant(output_float)\n",
        "\n",
        "        return output_quantized\n",
        "\n",
        "\n",
        "class MoEFFN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.quant = tq.QuantStub() # Quantization stub for the model's input\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.relu1 = nn.ReLU() # Changed from F.relu for fusibility\n",
        "        self.moe = MoELayer(256, 128, num_experts=4) # MoELayer itself handles quant/dequant internally\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.dequant = tq.DeQuantStub() # Dequantization stub for the model's output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x) # Quantize input\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu1(self.fc1(x)) # fc1 output -> relu1 output (will be quantized after fusion)\n",
        "        x = self.moe(x) # MoELayer takes quantized input, outputs quantized\n",
        "        x = self.fc3(x) # fc3 takes quantized input, outputs quantized\n",
        "        x = self.dequant(x) # Dequantize final output\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 4: Standard Training & Evaluation\n",
        "# ============================================================\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in tqdm(dataloader):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device=None):\n",
        "    if device is None:\n",
        "        device = DEVICE\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 5: Latency Measurement\n",
        "# ============================================================\n",
        "\n",
        "def measure_latency(model, dataloader, num_batches=50, device=None):\n",
        "    if device is None:\n",
        "        device = DEVICE\n",
        "    model.eval()\n",
        "\n",
        "    # Warm-up\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(dataloader):\n",
        "            model(x.to(device))\n",
        "            if i >= 5:\n",
        "                break\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(dataloader):\n",
        "            model(x.to(device))\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "\n",
        "    end = time.time()\n",
        "    return (end - start) / num_batches\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 6: Post-Training Quantization (PTQ)\n",
        "# ============================================================\n",
        "\n",
        "def apply_ptq(model, calibration_loader):\n",
        "    model.eval()\n",
        "    model.qconfig = tq.get_default_qconfig(\"qnnpack\")\n",
        "\n",
        "    # Move model to CPU before preparing and converting for QNNPACK,\n",
        "    # as QNNPACK typically uses CPU-optimized kernels.\n",
        "    model_for_quant = model.to(torch.device(\"cpu\"))\n",
        "\n",
        "    # Explicitly disable quantization for the internal modules of MoELayer\n",
        "    # as it's designed to operate as a float island internally.\n",
        "    model_for_quant.moe.gate.qconfig = None\n",
        "    for expert_seq in model_for_quant.moe.experts:\n",
        "        expert_seq[0].qconfig = None  # nn.Linear in expert\n",
        "        expert_seq[1].qconfig = None  # nn.ReLU in expert\n",
        "\n",
        "    # Fuse modules for better quantization performance.\n",
        "    # For MoEFFN's fc1 and relu1:\n",
        "    tq.fuse_modules(model_for_quant, [[\"fc1\", \"relu1\"]], inplace=True)\n",
        "    # MoELayer's internal layers (gate, experts) are now designed to operate in float\n",
        "    # due to explicit dequant/quant stubs in MoELayer, so no fusion is needed for them.\n",
        "\n",
        "    tq.prepare(model_for_quant, inplace=True)\n",
        "\n",
        "    # Calibration on CPU with CPU tensors\n",
        "    with torch.no_grad():\n",
        "        for x, _ in calibration_loader:\n",
        "            model_for_quant(x.to(torch.device(\"cpu\")))\n",
        "\n",
        "    tq.convert(model_for_quant, inplace=True)\n",
        "    return model_for_quant # Return the CPU-quantized model\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 7: Knowledge Distillation Training\n",
        "# ============================================================\n",
        "\n",
        "def train_kd(student, teacher, dataloader, optimizer, alpha=0.7, temperature=4.0):\n",
        "    student.train()\n",
        "    teacher.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in tqdm(dataloader):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = teacher(x)\n",
        "\n",
        "        student_logits = student(x)\n",
        "\n",
        "        hard_loss = F.cross_entropy(student_logits, y)\n",
        "\n",
        "        soft_loss = F.kl_div(\n",
        "            F.log_softmax(student_logits / temperature, dim=1),\n",
        "            F.softmax(teacher_logits / temperature, dim=1),\n",
        "            reduction=\"batchmean\"\n",
        "        ) * (temperature ** 2)\n",
        "\n",
        "        loss = alpha * soft_loss + (1 - alpha) * hard_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 8: Main Experiment Pipeline\n",
        "# ============================================================\n",
        "\n",
        "def main():\n",
        "    train_loader, test_loader = get_dataloaders()\n",
        "\n",
        "    # ---- Baseline ----\n",
        "    print(\"\\n=== Training Baseline ===\")\n",
        "    baseline = BaselineFFN().to(DEVICE)\n",
        "    opt = optim.Adam(baseline.parameters(), lr=LR)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        loss = train(baseline, train_loader, opt, criterion)\n",
        "        acc = evaluate(baseline, test_loader, device=DEVICE)\n",
        "        print(f\"Epoch {epoch+1}: Loss={loss:.4f}, Acc={acc:.4f}\")\n",
        "\n",
        "    base_acc = evaluate(baseline, test_loader, device=DEVICE)\n",
        "    base_lat = measure_latency(baseline, test_loader, device=DEVICE)\n",
        "\n",
        "    # ---- MoE ----\n",
        "    print(\"\\n=== Training MoE ===\")\n",
        "    moe = MoEFFN().to(DEVICE)\n",
        "    opt = optim.Adam(moe.parameters(), lr=LR)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        loss = train(moe, train_loader, opt, criterion)\n",
        "        acc = evaluate(moe, test_loader, device=DEVICE)\n",
        "        print(f\"Epoch {epoch+1}: Loss={loss:.4f}, Acc={acc:.4f}\")\n",
        "\n",
        "    moe_acc = evaluate(moe, test_loader, device=DEVICE)\n",
        "    moe_lat = measure_latency(moe, test_loader, device=DEVICE)\n",
        "\n",
        "    # ---- Knowledge Distillation ----\n",
        "    print(\"\\n=== Training MoE with Knowledge Distillation ===\")\n",
        "    kd_student = MoEFFN().to(DEVICE)\n",
        "    opt = optim.Adam(kd_student.parameters(), lr=LR)\n",
        "\n",
        "    for epoch in range(KD_EPOCHS):\n",
        "        loss = train_kd(kd_student, baseline, train_loader, opt)\n",
        "        acc = evaluate(kd_student, test_loader, device=DEVICE)\n",
        "        print(f\"KD Epoch {epoch+1}: Loss={loss:.4f}, Acc={acc:.4f}\")\n",
        "\n",
        "    kd_acc = evaluate(kd_student, test_loader, device=DEVICE)\n",
        "    kd_lat = measure_latency(kd_student, test_loader, device=DEVICE)\n",
        "\n",
        "    # ---- PTQ ----\n",
        "    calibration_subset = Subset(train_loader.dataset, range(1024))\n",
        "    calibration_loader = DataLoader(calibration_subset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Create a new MoEFFN instance for quantization, as applying PTQ modifies the model in-place\n",
        "    # and we want to quantize the KD student, not the original MoE model.\n",
        "    # Also, move to CPU before applying PTQ and fusion.\n",
        "    quant_kd_student = MoEFFN()\n",
        "    quant_kd_student.load_state_dict(kd_student.state_dict())\n",
        "    quant_moe = apply_ptq(quant_kd_student, calibration_loader)\n",
        "\n",
        "    quant_acc = evaluate(quant_moe, test_loader, device=torch.device(\"cpu\"))\n",
        "    quant_lat = measure_latency(quant_moe, test_loader, device=torch.device(\"cpu\"))\n",
        "\n",
        "    # ---- Final Results ----\n",
        "    print(\"\\n===== FINAL RESULTS ===\")\n",
        "    print(f\"Baseline      | Acc: {base_acc:.4f} | Latency: {base_lat:.6f}\")\n",
        "    print(f\"MoE           | Acc: {moe_acc:.4f} | Latency: {moe_lat:.6f}\")\n",
        "    print(f\"MoE + KD      | Acc: {kd_acc:.4f} | Latency: {kd_lat:.6f}\")\n",
        "    print(f\"MoE + KD + PTQ| Acc: {quant_acc:.4f} | Latency: {quant_lat:.6f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPsIRslt8HIR",
        "outputId": "e9a2966f-c4a1-4b17-915b-3cbe89ad016b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training Baseline ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:12<00:00, 75.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss=0.2888, Acc=0.9573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:12<00:00, 76.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss=0.1100, Acc=0.9684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:12<00:00, 75.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Loss=0.0723, Acc=0.9737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:12<00:00, 76.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Loss=0.0543, Acc=0.9785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:13<00:00, 68.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Loss=0.0382, Acc=0.9777\n",
            "\n",
            "=== Training MoE ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:14<00:00, 66.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss=0.2808, Acc=0.9606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:14<00:00, 62.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss=0.1104, Acc=0.9699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:18<00:00, 51.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Loss=0.0752, Acc=0.9713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:15<00:00, 61.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Loss=0.0544, Acc=0.9702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:15<00:00, 61.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Loss=0.0427, Acc=0.9763\n",
            "\n",
            "=== Training MoE with Knowledge Distillation ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:15<00:00, 61.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 1: Loss=1.5740, Acc=0.9624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:15<00:00, 59.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 2: Loss=0.2850, Acc=0.9704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:16<00:00, 56.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 3: Loss=0.1478, Acc=0.9738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:15<00:00, 59.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 4: Loss=0.1017, Acc=0.9765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:15<00:00, 58.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 5: Loss=0.0783, Acc=0.9785\n",
            "\n",
            "===== FINAL RESULTS ===\n",
            "Baseline      | Acc: 0.9777 | Latency: 0.007712\n",
            "MoE           | Acc: 0.9763 | Latency: 0.007964\n",
            "MoE + KD      | Acc: 0.9785 | Latency: 0.008079\n",
            "MoE + KD + PTQ| Acc: 0.9774 | Latency: 0.008207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SECTION 0: Imports & Global Configuration\n",
        "# ============================================================\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.quantization as tq\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Explicitly set the quantization engine to QNNPACK (standard for x86/ARM CPUs)\n",
        "if 'qnnpack' in torch.backends.quantized.supported_engines:\n",
        "    torch.backends.quantized.engine = 'qnnpack'\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "KD_EPOCHS = 5\n",
        "LR = 1e-3\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 1: Data Loading (MNIST)\n",
        "# ============================================================\n",
        "\n",
        "def get_dataloaders():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        # Normalization helps training stability\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.MNIST(\n",
        "        root=\"./data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    test_dataset = datasets.MNIST(\n",
        "        root=\"./data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 2: Baseline Feed-Forward Network\n",
        "# ============================================================\n",
        "\n",
        "class BaselineFFN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # flatten\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 3: Mixture of Experts (MoE)\n",
        "# ============================================================\n",
        "\n",
        "class MoELayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_experts=4):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "        # Quantization stubs: MoE Internal logic stays float (Float Island)\n",
        "        self.dequant = tq.DeQuantStub() # Dequantize input (int8 -> fp32)\n",
        "        self.quant = tq.QuantStub()     # Quantize output (fp32 -> int8)\n",
        "\n",
        "        # Router (gate)\n",
        "        self.gate = nn.Linear(input_dim, num_experts)\n",
        "\n",
        "        # Experts\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(input_dim, output_dim),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "            for _ in range(num_experts)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Dequantize incoming int8 tensor to float32\n",
        "        x_float = self.dequant(x)\n",
        "\n",
        "        # 2. Routing (Float32 operations)\n",
        "        gate_logits = self.gate(x_float)\n",
        "        gate_probs = F.softmax(gate_logits, dim=1)\n",
        "\n",
        "        # 3. Expert computation (Float32 operations)\n",
        "        expert_outputs_list = []\n",
        "        for expert in self.experts:\n",
        "            expert_outputs_list.append(expert(x_float))\n",
        "\n",
        "        expert_outputs = torch.stack(expert_outputs_list, dim=1)\n",
        "\n",
        "        # 4. Weighted sum (Float32 operations)\n",
        "        output_float = torch.sum(\n",
        "            gate_probs.unsqueeze(-1) * expert_outputs,\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        # 5. Quantize result back to int8 for the next layer\n",
        "        output_quantized = self.quant(output_float)\n",
        "\n",
        "        return output_quantized\n",
        "\n",
        "\n",
        "class MoEFFN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.quant = tq.QuantStub() # Input quantization\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.moe = MoELayer(256, 128, num_experts=4)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.dequant = tq.DeQuantStub() # Output dequantization\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.moe(x) # Enters float island, returns int8\n",
        "        x = self.fc3(x)\n",
        "        x = self.dequant(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 4: Standard Training & Evaluation\n",
        "# ============================================================\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device=None):\n",
        "    if device is None:\n",
        "        device = DEVICE\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 5: Latency & Model Size Measurements\n",
        "# ============================================================\n",
        "\n",
        "def measure_latency(model, dataloader, num_batches=50, device=None):\n",
        "    if device is None:\n",
        "        device = DEVICE\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Warm-up\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(dataloader):\n",
        "            model(x.to(device))\n",
        "            if i >= 5: break\n",
        "\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(dataloader):\n",
        "            model(x.to(device))\n",
        "            if i >= num_batches: break\n",
        "    end = time.time()\n",
        "\n",
        "    return (end - start) / num_batches\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"Counts total trainable parameters.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def estimate_model_size(model, quantized=False):\n",
        "    \"\"\"\n",
        "    Estimates model size in MB.\n",
        "    FP32 assumption: 4 bytes per param.\n",
        "    INT8 assumption: 1 byte per param.\n",
        "    \"\"\"\n",
        "    num_params = count_parameters(model)\n",
        "    # Note: This is a heuristic. Quantized models pack weights, but overhead exists.\n",
        "    bytes_per_param = 1 if quantized else 4\n",
        "    size_mb = (num_params * bytes_per_param) / (1024 ** 2)\n",
        "    return size_mb\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 6: Post-Training Quantization (PTQ)\n",
        "# ============================================================\n",
        "\n",
        "def apply_ptq(model, calibration_loader):\n",
        "    model.eval()\n",
        "    # 1. Set qconfig for the model (QNNPACK)\n",
        "    model.qconfig = tq.get_default_qconfig(\"qnnpack\")\n",
        "\n",
        "    # 2. Move to CPU (Quantization flow is typically CPU-based)\n",
        "    model_for_quant = model.to(torch.device(\"cpu\"))\n",
        "\n",
        "    # 3. Disable quantization for the MoE Internals (Float Island)\n",
        "    # We want MoE input/output to quantize, but the internal routing/experts to stay float.\n",
        "    model_for_quant.moe.gate.qconfig = None\n",
        "    for expert_seq in model_for_quant.moe.experts:\n",
        "        expert_seq[0].qconfig = None # nn.Linear\n",
        "        expert_seq[1].qconfig = None # nn.ReLU\n",
        "\n",
        "    # 4. Fuse modules (only layers that are NOT in the float island)\n",
        "    tq.fuse_modules(model_for_quant, [[\"fc1\", \"relu1\"]], inplace=True)\n",
        "\n",
        "    # 5. Prepare (Insert Observers)\n",
        "    tq.prepare(model_for_quant, inplace=True)\n",
        "\n",
        "    # 6. Calibrate\n",
        "    with torch.no_grad():\n",
        "        for x, _ in calibration_loader:\n",
        "            model_for_quant(x.to(torch.device(\"cpu\")))\n",
        "\n",
        "    # 7. Convert (Float -> Int8)\n",
        "    tq.convert(model_for_quant, inplace=True)\n",
        "\n",
        "    return model_for_quant\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 7: Knowledge Distillation Training\n",
        "# ============================================================\n",
        "\n",
        "def train_kd(student, teacher, dataloader, optimizer, alpha=0.7, temperature=4.0):\n",
        "    student.train()\n",
        "    teacher.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in tqdm(dataloader, desc=\"KD Training\", leave=False):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = teacher(x)\n",
        "\n",
        "        student_logits = student(x)\n",
        "\n",
        "        # Soft Target Loss (KL Divergence)\n",
        "        soft_loss = F.kl_div(\n",
        "            F.log_softmax(student_logits / temperature, dim=1),\n",
        "            F.softmax(teacher_logits / temperature, dim=1),\n",
        "            reduction=\"batchmean\"\n",
        "        ) * (temperature ** 2)\n",
        "\n",
        "        # Hard Target Loss (Cross Entropy)\n",
        "        hard_loss = F.cross_entropy(student_logits, y)\n",
        "\n",
        "        loss = alpha * soft_loss + (1 - alpha) * hard_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SECTION 8: Main Experiment Pipeline\n",
        "# ============================================================\n",
        "\n",
        "def main():\n",
        "    train_loader, test_loader = get_dataloaders()\n",
        "\n",
        "    # Calibration loader for PTQ\n",
        "    calibration_subset = Subset(train_loader.dataset, range(1024))\n",
        "    calibration_loader = DataLoader(calibration_subset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. Baseline\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\n=== 1. Training Baseline ===\")\n",
        "    baseline = BaselineFFN().to(DEVICE)\n",
        "    opt = optim.Adam(baseline.parameters(), lr=LR)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        loss = train(baseline, train_loader, opt, criterion)\n",
        "        print(f\"Epoch {epoch+1}: Loss={loss:.4f}\")\n",
        "\n",
        "    base_acc = evaluate(baseline, test_loader, device=DEVICE)\n",
        "    base_lat = measure_latency(baseline, test_loader, device=DEVICE)\n",
        "\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. MoE (Training from scratch)\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\n=== 2. Training MoE ===\")\n",
        "    moe = MoEFFN().to(DEVICE)\n",
        "    opt_moe = optim.Adam(moe.parameters(), lr=LR)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        loss = train(moe, train_loader, opt_moe, criterion)\n",
        "        print(f\"Epoch {epoch+1}: Loss={loss:.4f}\")\n",
        "\n",
        "    moe_acc = evaluate(moe, test_loader, device=DEVICE)\n",
        "    moe_lat = measure_latency(moe, test_loader, device=DEVICE)\n",
        "\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. MoE + KD\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\n=== 3. Training MoE with Knowledge Distillation ===\")\n",
        "    kd_student = MoEFFN().to(DEVICE)\n",
        "    opt_kd = optim.Adam(kd_student.parameters(), lr=LR)\n",
        "\n",
        "    for epoch in range(KD_EPOCHS):\n",
        "        loss = train_kd(kd_student, baseline, train_loader, opt_kd)\n",
        "        print(f\"KD Epoch {epoch+1}: Loss={loss:.4f}\")\n",
        "\n",
        "    kd_acc = evaluate(kd_student, test_loader, device=DEVICE)\n",
        "    kd_lat = measure_latency(kd_student, test_loader, device=DEVICE)\n",
        "\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 4. MoE + KD + PTQ\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\n=== 4. Applying PTQ to MoE  ===\")\n",
        "    quant_kd_student = MoEFFN()\n",
        "    quant_kd_student.load_state_dict(kd_student.state_dict())\n",
        "\n",
        "    # Apply PTQ (Helper function handles the float island)\n",
        "    quant_moe = apply_ptq(quant_kd_student, calibration_loader)\n",
        "\n",
        "    quant_acc = evaluate(quant_moe, test_loader, device=torch.device(\"cpu\"))\n",
        "    quant_lat = measure_latency(quant_moe, test_loader, device=torch.device(\"cpu\"))\n",
        "\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # Final Results\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"{'Model':<20} | {'Acc':<8} | {'Lat(ms)':<8} | {'Params':<12} | {'Size(MB)':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # 1. Baseline\n",
        "    print(f\"{'Baseline':<20} | {base_acc:.4f}   | {base_lat*1000:.2f}     | {count_parameters(baseline):<12,} | {estimate_model_size(baseline):.2f}\")\n",
        "\n",
        "    # 2. MoE\n",
        "    print(f\"{'MoE':<20} | {moe_acc:.4f}   | {moe_lat*1000:.2f}     | {count_parameters(moe):<12,} | {estimate_model_size(moe):.2f}\")\n",
        "\n",
        "    # 3. MoE + KD\n",
        "    print(f\"{'MoE + KD':<20} | {kd_acc:.4f}   | {kd_lat*1000:.2f}     | {count_parameters(kd_student):<12,} | {estimate_model_size(kd_student):.2f}\")\n",
        "\n",
        "    # 4. MoE + KD + PTQ\n",
        "    print(f\"{'MoE + KD + PTQ':<20} | {quant_acc:.4f}   | {quant_lat*1000:.2f}     | {count_parameters(quant_moe):<12,} | {estimate_model_size(quant_moe, True):.2f}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4lO7k168hKQ",
        "outputId": "18d04264-57e8-488a-c627-86ad5f3e06d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 1. Training Baseline ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss=0.2277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss=0.0935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Loss=0.0656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Loss=0.0485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Loss=0.0407\n",
            "\n",
            "=== 2. Training MoE ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss=0.2332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss=0.1012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Loss=0.0721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Loss=0.0530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Loss=0.0446\n",
            "\n",
            "=== 3. Training MoE with Knowledge Distillation ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 1: Loss=1.3546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 2: Loss=0.2368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 3: Loss=0.1475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 4: Loss=0.1147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KD Epoch 5: Loss=0.0985\n",
            "\n",
            "=== 4. Applying PTQ to MoE  ===\n",
            "\n",
            "================================================================================\n",
            "Model                | Acc      | Lat(ms)  | Params       | Size(MB)  \n",
            "--------------------------------------------------------------------------------\n",
            "Baseline             | 0.9742   | 14.87     | 235,146      | 0.90\n",
            "MoE                  | 0.9751   | 13.96     | 334,862      | 1.28\n",
            "MoE + KD             | 0.9762   | 13.12     | 334,862      | 1.28\n",
            "MoE + KD + PTQ       | 0.9760   | 17.03     | 132,612      | 0.13\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch -q"
      ],
      "metadata": {
        "id": "LRvX_x0s-D9d"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.quantization as tq\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------------\n",
        "# Global Config (Colab Safe)\n",
        "# -------------------------------\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 1\n",
        "LR = 1e-3\n",
        "NUM_EXPERTS = 4\n",
        "\n",
        "engines = torch.backends.quantized.supported_engines\n",
        "print(f\"Supported engines: {engines}\")\n",
        "\n",
        "# ============================================================\n",
        "# Utils\n",
        "# ============================================================\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def estimate_model_size(model, quantized=False):\n",
        "    bytes_per_param = 1 if quantized else 4\n",
        "    size_mb = count_parameters(model) * bytes_per_param / (1024**2)\n",
        "    return size_mb\n",
        "\n",
        "def measure_latency(model, dataloader, num_batches=50):\n",
        "    model.eval()\n",
        "\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(dataloader):\n",
        "            _ = model(x)\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "    end = time.time()\n",
        "\n",
        "    return (end - start) / num_batches\n",
        "\n",
        "# ============================================================\n",
        "# Data\n",
        "# ============================================================\n",
        "\n",
        "def get_dataloaders():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "    train_ds = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
        "    test_ds  = datasets.MNIST(\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    calib_subset = Subset(train_ds, range(1024))\n",
        "    calib_loader = DataLoader(calib_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader, calib_loader\n",
        "\n",
        "# ============================================================\n",
        "# Baseline\n",
        "# ============================================================\n",
        "\n",
        "class BaselineFFN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.quant = tq.QuantStub()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.dequant = tq.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.quant(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        # x = self.dequant(x)\n",
        "        return x\n",
        "\n",
        "# ============================================================\n",
        "# MoE\n",
        "# ============================================================\n",
        "\n",
        "class MoELayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_experts):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "        self.dequant = tq.DeQuantStub()\n",
        "        self.quant = tq.QuantStub()\n",
        "\n",
        "        self.gate = nn.Linear(input_dim, num_experts)\n",
        "\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(input_dim, output_dim),\n",
        "                nn.ReLU()\n",
        "            ) for _ in range(num_experts)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_float = self.dequant(x)\n",
        "\n",
        "        gate_logits = self.gate(x_float)\n",
        "        gate_probs = F.softmax(gate_logits, dim=1)\n",
        "\n",
        "        expert_outs = []\n",
        "        for expert in self.experts:\n",
        "            expert_outs.append(expert(x_float))\n",
        "\n",
        "        expert_outs = torch.stack(expert_outs, dim=1)\n",
        "\n",
        "        out_float = torch.sum(gate_probs.unsqueeze(-1) * expert_outs, dim=1)\n",
        "        out_quant = self.quant(out_float)\n",
        "\n",
        "        return out_quant, gate_probs\n",
        "\n",
        "\n",
        "class MoEFFN(nn.Module):\n",
        "    def __init__(self, num_experts):\n",
        "        super().__init__()\n",
        "        self.quant = tq.QuantStub()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.moe = MoELayer(256, 128, num_experts)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.dequant = tq.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x, gate_probs = self.moe(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.dequant(x)\n",
        "        return x, gate_probs\n",
        "\n",
        "# ============================================================\n",
        "# Training\n",
        "# ============================================================\n",
        "\n",
        "def train_baseline(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for x, y in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    return total_loss / len(dataloader), correct / total\n",
        "\n",
        "\n",
        "def train_moe(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for x, y in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    return total_loss / len(dataloader), correct / total\n",
        "\n",
        "\n",
        "def train_kd(student, teacher, dataloader, optimizer, alpha=0.7, temperature=4.0):\n",
        "    student.train()\n",
        "    teacher.eval()\n",
        "\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for x, y in tqdm(dataloader, desc=\"KD Training\", leave=False):\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = teacher(x)\n",
        "\n",
        "        student_logits, _ = student(x)\n",
        "\n",
        "        soft_loss = F.kl_div(\n",
        "            F.log_softmax(student_logits / temperature, dim=1),\n",
        "            F.softmax(teacher_logits / temperature, dim=1),\n",
        "            reduction=\"batchmean\"\n",
        "        ) * (temperature ** 2)\n",
        "\n",
        "        hard_loss = F.cross_entropy(student_logits, y)\n",
        "        loss = alpha * soft_loss + (1 - alpha) * hard_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = student_logits.argmax(1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    return total_loss / len(dataloader), correct / total\n",
        "\n",
        "# ============================================================\n",
        "# Eval\n",
        "# ============================================================\n",
        "\n",
        "def eval_baseline(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            preds = model(x).argmax(1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def eval_moe(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            logits, _ = model(x)\n",
        "            preds = logits.argmax(1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# ============================================================\n",
        "# Router Load\n",
        "# ============================================================\n",
        "\n",
        "def router_load_analysis(model, loader, num_batches=100):\n",
        "    model.eval()\n",
        "    usage = torch.zeros(NUM_EXPERTS)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(loader):\n",
        "            _, gate_probs = model(x)\n",
        "            top = gate_probs.argmax(1)\n",
        "            for e in top:\n",
        "                usage[e] += 1\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "\n",
        "    return usage / usage.sum()\n",
        "\n",
        "# ============================================================\n",
        "# PTQ\n",
        "# ============================================================\n",
        "\n",
        "def apply_ptq(model, calib_loader):\n",
        "    model.eval()\n",
        "    model.qconfig = tq.get_default_qconfig(\"qnnpack\")\n",
        "\n",
        "    # Float island\n",
        "    model.moe.gate.qconfig = None\n",
        "    for ex in model.moe.experts:\n",
        "        ex[0].qconfig = None\n",
        "        ex[1].qconfig = None\n",
        "\n",
        "    tq.fuse_modules(model, [[\"fc1\",\"relu1\"]], inplace=True)\n",
        "\n",
        "    tq.prepare(model, inplace=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, _ in calib_loader:\n",
        "            model(x)\n",
        "\n",
        "    tq.convert(model, inplace=True)\n",
        "    return model\n",
        "\n",
        "# ============================================================\n",
        "# Main\n",
        "# ============================================================\n",
        "\n",
        "def main():\n",
        "    train_loader, test_loader, calib_loader = get_dataloaders()\n",
        "\n",
        "    # 1. Baseline\n",
        "    baseline = BaselineFFN()\n",
        "    opt = optim.Adam(baseline.parameters(), lr=LR)\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"\\n---1. Baseline training ...\")\n",
        "    for e in range(EPOCHS):\n",
        "        loss, acc = train_baseline(baseline, train_loader, opt, crit)\n",
        "        print(f\"Epoch {e+1} | Loss: {loss:.4f} | Accuracy: {acc:.4f}\")\n",
        "\n",
        "    base_acc = eval_baseline(baseline, test_loader)\n",
        "    base_lat = measure_latency(baseline, test_loader)\n",
        "\n",
        "    # 2. MoE\n",
        "    moe = MoEFFN(NUM_EXPERTS)\n",
        "    opt_moe = optim.Adam(moe.parameters(), lr=LR)\n",
        "\n",
        "    print(\"\\n---2. MoE training ...\")\n",
        "    for e in range(EPOCHS):\n",
        "        loss, acc = train_moe(moe, train_loader, opt_moe, crit)\n",
        "        print(f\"Epoch {e+1} | Loss: {loss:.4f} | Accuracy: {acc:.4f}\")\n",
        "\n",
        "    moe_acc = eval_moe(moe, test_loader)\n",
        "    moe_lat = measure_latency(moe, test_loader)\n",
        "\n",
        "    # 3. KD\n",
        "    kd_student = MoEFFN(NUM_EXPERTS)\n",
        "    opt_kd = optim.Adam(kd_student.parameters(), lr=LR)\n",
        "\n",
        "    print(\"\\n---3. KD training ...\")\n",
        "    for e in range(EPOCHS):\n",
        "        loss, acc = train_kd(kd_student, baseline, train_loader, opt_kd)\n",
        "        print(f\"Epoch {e+1} | Loss: {loss:.4f} | Accuracy: {acc:.4f}\")\n",
        "\n",
        "    kd_acc = eval_moe(kd_student, test_loader)\n",
        "    kd_lat = measure_latency(kd_student, test_loader)\n",
        "\n",
        "    # 4. PTQ\n",
        "    quant_moe = MoEFFN(NUM_EXPERTS)\n",
        "    quant_moe.load_state_dict(moe.state_dict())\n",
        "    quant_moe = apply_ptq(quant_moe, calib_loader)\n",
        "\n",
        "    quant_acc = eval_moe(quant_moe, test_loader)\n",
        "    quant_lat = measure_latency(quant_moe, test_loader)\n",
        "\n",
        "    quant_kd_student = MoEFFN(NUM_EXPERTS)\n",
        "    quant_kd_student.load_state_dict(kd_student.state_dict())\n",
        "\n",
        "    quant_moe_kd = apply_ptq(quant_kd_student, calib_loader)\n",
        "    quant_acc_kd = eval_moe(quant_moe_kd, test_loader)\n",
        "    quant_lat_kd = measure_latency(quant_moe_kd, test_loader)\n",
        "\n",
        "\n",
        "\n",
        "    # Final table\n",
        "    print(\"....Final table...\")\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"{'Model':<20} | {'Acc':<8} | {'Lat(ms)':<8} | {'Params':<12} | {'Size(MB)':<10}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    print(f\"{'Baseline':<20} | {base_acc:.4f} | {base_lat*1000:.2f} | {count_parameters(baseline):<12,} | {estimate_model_size(baseline):.2f}\")\n",
        "    print(f\"{'MoE':<20} | {moe_acc:.4f} | {moe_lat*1000:.2f} | {count_parameters(moe):<12,} | {estimate_model_size(moe):.2f}\")\n",
        "    print(f\"{'MoE + PTQ':<20} | {quant_acc:.4f} | {quant_lat*1000:.2f} | {count_parameters(quant_moe):<12,} | {estimate_model_size(quant_moe, True):.2f}\")\n",
        "    print(f\"{'MoE + KD':<20} | {kd_acc:.4f} | {kd_lat*1000:.2f} | {count_parameters(kd_student):<12,} | {estimate_model_size(kd_student):.2f}\")\n",
        "    print(f\"{'MoE + KD + PTQ':<20} | {quant_acc_kd:.4f} | {quant_lat_kd*1000:.2f} | {count_parameters(quant_moe_kd):<12,} | {estimate_model_size(quant_moe_kd, True):.2f}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    moe_load = router_load_analysis(moe, test_loader)\n",
        "    print(\"\\nRouter Load Distribution (MoE):\")\n",
        "    for i, v in enumerate(moe_load):\n",
        "        print(f\"Expert {i}: {v.item()*100:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1OkK8ETp6yy",
        "outputId": "1311ccb1-ded0-409a-ac05-7554ad9dcd42"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supported engines: ['qnnpack', 'onednn', 'x86', 'fbgemm']\n",
            "\n",
            "---1. Baseline training ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 0.2286 | Accuracy: 0.9306\n",
            "\n",
            "---2. MoE training ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 0.2317 | Accuracy: 0.9308\n",
            "\n",
            "---3. KD training ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 0.4378 | Accuracy: 0.9306\n",
            "....Final table...\n",
            "\n",
            "======================================================================\n",
            "Model                | Acc      | Lat(ms)  | Params       | Size(MB)  \n",
            "----------------------------------------------------------------------\n",
            "Baseline             | 0.9601 | 13.60 | 235,146      | 0.90\n",
            "MoE                  | 0.9547 | 13.76 | 334,862      | 1.28\n",
            "MoE + PTQ            | 0.9546 | 16.65 | 132,612      | 0.13\n",
            "MoE + KD             | 0.9579 | 14.07 | 334,862      | 1.28\n",
            "MoE + KD + PTQ       | 0.9586 | 33.99 | 132,612      | 0.13\n",
            "======================================================================\n",
            "\n",
            "Router Load Distribution (MoE):\n",
            "Expert 0: 0.14%\n",
            "Expert 1: 0.00%\n",
            "Expert 2: 97.66%\n",
            "Expert 3: 2.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mKJlPOvyx9eh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}